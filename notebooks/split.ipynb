{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as infile:\n",
    "        return infile.read()\n",
    "    \n",
    "def save_file(filepath, content):\n",
    "    with open(filepath, 'w', encoding='utf-8') as outfile:\n",
    "        outfile.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def count_openai_tokens(input_string: str, model: str = \"gpt-4o\") -> int:\n",
    "    \"\"\"\n",
    "    Calculate the total number of OpenAI tokens a string will consume.\n",
    "\n",
    "    Args:\n",
    "        input_string (str): The string to tokenize.\n",
    "        model (str): The OpenAI model to use for tokenization.\n",
    "                     Defaults to \"gpt-3.5-turbo\".\n",
    "\n",
    "    Returns:\n",
    "        int: The total number of tokens consumed by the string.\n",
    "    \"\"\"\n",
    "    # Load the tokenizer for the specified model\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    \n",
    "    # Encode the input string to tokens\n",
    "    tokens = encoding.encode(input_string)\n",
    "    \n",
    "    # Return the token count\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open_file(\"../docs/truein/split/truein.txt_Part_105\")\n",
    "count_openai_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import uuid\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Load the tokenizer for the specified model\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = encoding.encode(text, disallowed_special=())\n",
    "    return len(tokens)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=7500, # the maximum for text-embedding-3-small is 8191\n",
    "    chunk_overlap=100,\n",
    "    length_function=tiktoken_len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "# get a UUID - URL safe, Base64 for the split documents we need an iid for each\n",
    "def get_a_uuid():\n",
    "    return str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def split_file_to_df(input_dir: str, category: str, output_dir: str):\n",
    "\n",
    "    #directory = \"../docs/full\"\n",
    "    chunk = {}\n",
    "    txt = []\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(input_dir, filename), \"r\", encoding='UTF-8') as f:\n",
    "                text = f.read()\n",
    "                texts = text_splitter.create_documents([text])\n",
    "                doc_count = 0\n",
    "                for i in texts:\n",
    "                    chunk = {\n",
    "                            \"id\": get_a_uuid(),  # generate a random uuid for the document\n",
    "                            \"title\": f\"{filename[:-4]}_Part_{doc_count}\",  # remove the .txt extension from the filename and use this as the title\n",
    "                            \"content\": i.page_content,\n",
    "                            \"sourcefile\": filename,\n",
    "                            \"content_tokens\": count_openai_tokens(i.page_content, \"gpt-4o\"),\n",
    "                            \"category\": category,\n",
    "                            \"contentVector\": get_embedding(i.page_content)\n",
    "                            }\n",
    "                    txt.append(chunk)\n",
    "                    doc_count += 1\n",
    "                    save_file(f\"{output_dir}/{filename[:-4]}_Part_{doc_count}.txt\", i.page_content)\n",
    "\n",
    "    df = pd.DataFrame(txt)\n",
    "\n",
    "    # Get the total number of rows\n",
    "    total_rows = len(df)\n",
    "    print(\"Total number of rows:\", total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def split_file_to_files(input_dir: str, output_dir: str):\n",
    "\n",
    "    #directory = \"../docs/full\"\n",
    "    txt = []\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(input_dir, filename), \"r\", encoding='UTF-8') as f:\n",
    "                text = f.read()\n",
    "                texts = text_splitter.create_documents([text])  # create chunks of smaller documents based on text_splitter parameters\n",
    "                doc_count = 0\n",
    "                for i in texts:\n",
    "                    txt.append(i.page_content)\n",
    "                    doc_count += 1\n",
    "                    save_file(f\"{output_dir}/{filename[:-4]}_Part_{doc_count}.txt\", i.page_content)\n",
    "\n",
    "    df = pd.DataFrame(txt)\n",
    "\n",
    "    # Get the total number of rows\n",
    "    total_rows = len(df)\n",
    "    print(\"Total number of rows:\", total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_file_to_files(\"../docs/northside/\", \"../docs/northside/split/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
